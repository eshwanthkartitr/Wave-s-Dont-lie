{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTEEGBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4, head_dim=None, eta=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim if head_dim is not None else hidden_size // num_heads\n",
    "        self.eta = eta\n",
    "        \n",
    "        self.W1 = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim) * 0.02)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_heads, 1, self.head_dim))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, cache_params=None):\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        x = x.reshape(B, L, self.num_heads, self.head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)  # [B, H, L, D]\n",
    "        \n",
    "        if cache_params is None:\n",
    "            output = self.dropout(torch.einsum('bhld,hdf->bhlf', x, self.W1) + self.b1.unsqueeze(0))\n",
    "        else:\n",
    "            with torch.enable_grad():\n",
    "                x_temp = x.detach().requires_grad_()\n",
    "                output_temp = torch.einsum('bhld,hdf->bhlf', x_temp, self.W1) + self.b1.unsqueeze(0)\n",
    "                \n",
    "                # Self-supervised loss: L2 loss + consistency loss\n",
    "                l2_loss = output_temp.pow(2).mean()\n",
    "                consistency_loss = torch.abs(output_temp - x_temp.mean(dim=-1, keepdim=True)).mean()\n",
    "                self_supervised_loss = l2_loss + 0.1 * consistency_loss\n",
    "                \n",
    "                # Compute gradients for self-supervised loss\n",
    "                grads = torch.autograd.grad(self_supervised_loss, [self.W1, self.b1], retain_graph=True, create_graph=False)\n",
    "                grad_W1, grad_b1 = grads\n",
    "                \n",
    "                grad_W1 = torch.clamp(grad_W1, -1.0, 1.0)\n",
    "                grad_b1 = torch.clamp(grad_b1, -1.0, 1.0)\n",
    "                \n",
    "                if f'W1_states_{id(self)}' not in cache_params:\n",
    "                    cache_params[f'W1_states_{id(self)}'] = self.W1.clone()\n",
    "                    cache_params[f'b1_states_{id(self)}'] = self.b1.clone()\n",
    "                    cache_params[f'momentum_W1_{id(self)}'] = torch.zeros_like(self.W1)\n",
    "                    cache_params[f'momentum_b1_{id(self)}'] = torch.zeros_like(self.b1)\n",
    "                \n",
    "                momentum = 0.9\n",
    "                cache_params[f'momentum_W1_{id(self)}'] = (\n",
    "                    momentum * cache_params[f'momentum_W1_{id(self)}'] + \n",
    "                    (1 - momentum) * grad_W1\n",
    "                )\n",
    "                cache_params[f'momentum_b1_{id(self)}'] = (\n",
    "                    momentum * cache_params[f'momentum_b1_{id(self)}'] + \n",
    "                    (1 - momentum) * grad_b1\n",
    "                )\n",
    "                \n",
    "                cache_params[f'W1_states_{id(self)}'] -= self.eta * cache_params[f'momentum_W1_{id(self)}']\n",
    "                cache_params[f'b1_states_{id(self)}'] -= self.eta * cache_params[f'momentum_b1_{id(self)}']\n",
    "                \n",
    "                output = self.dropout(torch.einsum('bhld,hdf->bhlf', x, \n",
    "                                    cache_params[f'W1_states_{id(self)}']) + \n",
    "                                    cache_params[f'b1_states_{id(self)}'].unsqueeze(0))\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3)  # [B, L, H, D]\n",
    "        output = output.reshape(B, L, -1)     # [B, L, H*D]\n",
    "        \n",
    "        return output + x.permute(0, 2, 1, 3).reshape(B, L, -1)  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EEG data from .mat files\n",
    "def load_mat_data(file_path):\n",
    "    data = loadmat(file_path)['data'][0]\n",
    "    inputs, targets = [], []\n",
    "    map = {}\n",
    "    current = 3\n",
    "    for i in range(len(data)):\n",
    "        y = data[i].flatten()\n",
    "        sub = y[0][0]\n",
    "        if y[1][0]=='letter-composing':\n",
    "            continue\n",
    "        if y[2][0] not in ['trial 1','trial 2','trial 3']:\n",
    "            continue\n",
    "        eeg_data = y[3][:6, :]\n",
    "        if sub not in map:\n",
    "            map[sub] = current\n",
    "            current += 1\n",
    "        targets.append(map[sub])\n",
    "        inputs.append(eeg_data)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "def preprocess_eeg(eeg_data):\n",
    "    eeg_data = (eeg_data - np.mean(eeg_data)) / (np.std(eeg_data) + 1e-8)\n",
    "    nyq = 0.5 * 256\n",
    "    low, high = 0.5 / nyq, 45 / nyq\n",
    "    b, a = butter(4, [low, high], btype='band')\n",
    "    return filtfilt(b, a, eeg_data)\n",
    "\n",
    "# Load EEG data from folder structure\n",
    "def load_folder_data(root_path, tasks=['baseline', 'counting', 'logical', 'rotation']):\n",
    "    root = Path(root_path)\n",
    "    X, y = [], []\n",
    "    subjects = [d for d in root.iterdir() if d.is_dir() and d.name in ['eshwa', 'Giri', 'Nithish']]\n",
    "    for subject_idx, subject in enumerate(subjects):\n",
    "        for task in tasks:\n",
    "            session_pattern = f\"OpenBCISession_{subject.name.lower()}_{task}_*\"\n",
    "            sessions = list(subject.glob(session_pattern))\n",
    "            for session in sessions:\n",
    "                raw_files = list(session.glob(\"OpenBCI-RAW-*.txt\"))\n",
    "                for raw_file in raw_files:\n",
    "                    eeg_data = load_eeg_data(raw_file)\n",
    "                    if eeg_data is not None:\n",
    "                        for i in range(6):\n",
    "                            X.append(eeg_data[3750:6250, i])\n",
    "                            y.append(subject_idx)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def jitter(x, sigma=0.01):\n",
    "    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    factor = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], 1))\n",
    "    return x * factor\n",
    "\n",
    "\n",
    "def create_spectrogram(eeg_data, fs=256, target_area=2500, num_frequencies=50, num_times=50):   \n",
    "    # Calculate the segment length (nperseg) based on the desired number of frequency bins\n",
    "    # The formula for frequency bins is: number of bins = nperseg / 2 + 1\n",
    "    required_nperseg = (num_frequencies - 1) * 2  # Desired frequency bins and corresponding nperseg\n",
    "\n",
    "    # Ensure nperseg doesn't exceed the total signal length\n",
    "    nperseg = min(required_nperseg, len(eeg_data))\n",
    "\n",
    "    noverlap = nperseg // 2  # Overlap (half of segment length by default)\n",
    "\n",
    "    # Compute the spectrogram\n",
    "    frequencies, times, Sxx = spectrogram(\n",
    "        eeg_data, fs=fs, window='hamming', nperseg=nperseg, noverlap=noverlap, detrend='constant', scaling='density'\n",
    "    )\n",
    "\n",
    "    # Ensure we get the expected number of frequency bins\n",
    "    if Sxx.shape[0] < num_frequencies:\n",
    "        print(f\"Warning: Expected {num_frequencies} frequency bins, but got {Sxx.shape[0]}. Padding with zeros.\")\n",
    "        padding = num_frequencies - Sxx.shape[0]\n",
    "        Sxx = np.pad(Sxx, ((0, padding), (0, 0)), mode='constant')\n",
    "    elif Sxx.shape[0] > num_frequencies:\n",
    "        # If the number of frequency bins is more than desired, truncate\n",
    "        Sxx = Sxx[:num_frequencies, :]\n",
    "\n",
    "    # Ensure the number of time bins matches the desired number\n",
    "    if Sxx.shape[1] < num_times:\n",
    "        padding = num_times - Sxx.shape[1]\n",
    "        Sxx = np.pad(Sxx, ((0, 0), (0, padding)), mode='constant')\n",
    "    elif Sxx.shape[1] > num_times:\n",
    "        # Trim time bins if there are more than desired\n",
    "        Sxx = Sxx[:, :num_times]\n",
    "\n",
    "    # Convert to decibels\n",
    "    Sxx = 10 * np.log10(Sxx + 1e-5)\n",
    "\n",
    "    # Normalize the spectrogram\n",
    "    Sxx = (Sxx - Sxx.min()) / (Sxx.max() - Sxx.min() + 1e-8)\n",
    "    return Sxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TTTCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, eta=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Hidden state model (f in the paper)\n",
    "        self.hidden_model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize cache for test-time updates\n",
    "        self.cache_params = {}\n",
    "        \n",
    "    def forward(self, x, hidden, is_training=True):\n",
    "        if is_training:\n",
    "            next_hidden = self.hidden_model(x)\n",
    "        else:\n",
    "            # Test-time training update\n",
    "            with torch.enable_grad():\n",
    "                x_temp = x.detach().requires_grad_()\n",
    "                pred = self.hidden_model(x_temp)\n",
    "                \n",
    "                # Self-supervised loss (as described in paper section 2.1)\n",
    "                l2_loss = pred.pow(2).mean()\n",
    "                consistency_loss = torch.abs(pred - x_temp.mean(dim=-1, keepdim=True)).mean()\n",
    "                loss = l2_loss + 0.1 * consistency_loss\n",
    "                \n",
    "                # Update hidden model parameters\n",
    "                grads = torch.autograd.grad(loss, self.hidden_model.parameters())\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for param, grad in zip(self.hidden_model.parameters(), grads):\n",
    "                        if f'momentum_{id(param)}' not in self.cache_params:\n",
    "                            self.cache_params[f'momentum_{id(param)}'] = torch.zeros_like(param)\n",
    "                        \n",
    "                        # Momentum update\n",
    "                        self.cache_params[f'momentum_{id(param)}'].mul_(0.9).add_(grad, alpha=0.1)\n",
    "                        param.add_(self.cache_params[f'momentum_{id(param)}'], alpha=-self.eta)\n",
    "                \n",
    "                next_hidden = self.hidden_model(x)\n",
    "                \n",
    "        return next_hidden\n",
    "\n",
    "\n",
    "class TTTRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create TTT cells for each layer\n",
    "        self.cells = nn.ModuleList([\n",
    "            TTTCell(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                hidden_size\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, hidden=None, is_training=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = [torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                     for _ in range(self.num_layers)]\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            layer_input = x[:, t, :]\n",
    "            for layer_idx, cell in enumerate(self.cells):\n",
    "                hidden[layer_idx] = cell(layer_input, hidden[layer_idx], is_training)\n",
    "                layer_input = hidden[layer_idx]\n",
    "            outputs.append(layer_input)\n",
    "            \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=10, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.cache_params = {}\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Initial block\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 25x25\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 12x12\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 6x6\n",
    "            \n",
    "            # Fourth block with global pooling\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling to 1x1\n",
    "        )\n",
    "        \n",
    "        # TTT blocks\n",
    "        self.ttt_blocks = nn.ModuleList([\n",
    "            TTTEEGBlock(hidden_size=256, num_heads=4)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),  # Now using 256 features from global pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Reconstruction decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 1024),  # From 256 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 50 * 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Print input shape for debugging\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        # Ensure input shape [B, C, H, W]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)  # [B, 256, 1, 1]\n",
    "        # print(\"After feature extractor:\", features.shape)\n",
    "        \n",
    "        # Reshape for TTT blocks\n",
    "        features = features.squeeze(-1).squeeze(-1)  # [B, 256]\n",
    "        features = features.unsqueeze(1)  # [B, 1, 256]\n",
    "        \n",
    "        # Apply TTT blocks\n",
    "        for block in self.ttt_blocks:\n",
    "            features = block(features, self.cache_params)\n",
    "        \n",
    "        # Prepare for classification\n",
    "        features = features.squeeze(1)  # [B, 256]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon = self.decoder(features)\n",
    "        recon = recon.view(-1, 1, 50, 50)\n",
    "        \n",
    "        return logits, recon\n",
    "    \n",
    "def SEBlock(channels):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Conv2d(channels, channels, kernel_size=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "def ResidualBlock(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.GELU(),\n",
    "        SEBlock(out_channels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "#         optimizer,\n",
    "#         max_lr=learning_rate,\n",
    "#         epochs=num_epochs,\n",
    "#         steps_per_epoch=len(train_loader)\n",
    "#     )\n",
    "#     criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     patience = 7\n",
    "#     patience_counter = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "        \n",
    "#         pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "#         for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "#             inputs = inputs.float()\n",
    "            \n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "#             try:\n",
    "#                 # Forward pass\n",
    "#                 outputs, recon = model(inputs)\n",
    "                \n",
    "#                 # Calculate losses\n",
    "#                 ce_loss = criterion(outputs, labels)\n",
    "#                 recon_loss = F.mse_loss(recon, inputs)\n",
    "#                 loss = ce_loss + 0.1 * recon_loss\n",
    "                \n",
    "#                 # Backward pass\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "                \n",
    "#                 # Update metrics\n",
    "#                 running_loss += loss.item()\n",
    "#                 _, predicted = outputs.max(1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "#                 # Update progress bar\n",
    "#                 accuracy = 100. * correct / total\n",
    "#                 pbar.set_postfix({\n",
    "#                     'loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "#                     'acc': f'{accuracy:.2f}%'\n",
    "#                 })\n",
    "                \n",
    "#             except RuntimeError as e:\n",
    "#                 print(f\"Error in batch {batch_idx}: {e}\")\n",
    "#                 print(f\"Input shape: {inputs.shape}\")\n",
    "#                 raise e\n",
    "        \n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         val_loss = 0.0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs = inputs.float()\n",
    "#                 outputs, _ = model(inputs)\n",
    "#                 _, predicted = outputs.max(1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "#         val_accuracy = 100. * val_correct / val_total\n",
    "#         print(f'\\nValidation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "#         # Early stopping\n",
    "#         if val_accuracy > best_val_acc:\n",
    "#             best_val_acc = val_accuracy\n",
    "#             patience_counter = 0\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'val_acc': val_accuracy,\n",
    "#             }, 'best_model.pth')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= patience:\n",
    "#                 print(f'Early stopping triggered. Best validation accuracy: {best_val_acc:.2f}%')\n",
    "#                 checkpoint = torch.load('best_model.pth')\n",
    "#                 model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#                 break\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# Test-time adaptation function\n",
    "def test_time_adaptation(model, test_loader, adaptation_steps=5, adaptation_lr=0.001):\n",
    "    model.eval()\n",
    "    inner_optimizer = optim.SGD(model.inner_model.parameters(), lr=adaptation_lr)\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        for _ in range(adaptation_steps):\n",
    "            inner_optimizer.zero_grad()\n",
    "            _, recon = model(inputs)\n",
    "            inputs = inputs.squeeze(2)\n",
    "            recon_loss = nn.functional.mse_loss(recon, inputs)\n",
    "            inputs = inputs.unsqueeze(2)\n",
    "            recon_loss.backward()\n",
    "            inner_optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 1. Reduce learning rate and add weight decay\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,  # Reduced from 0.0003\n",
    "        weight_decay=0.05,  # Increased from 0.01\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # 2. Modified learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # 3. Enhanced loss function\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.2)  # Increased smoothing\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    \n",
    "    # 4. Add early stopping\n",
    "    best_val_acc = 0\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        hidden = None\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 5. Mixup augmentation\n",
    "            alpha = 0.2\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            batch_size = inputs.size()[0]\n",
    "            index = torch.randperm(batch_size).to(device)\n",
    "            mixed_x = lam * inputs + (1 - lam) * inputs[index, :]\n",
    "            target_a, target_b = targets, targets[index]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, recon, hidden = model(mixed_x, hidden)\n",
    "            hidden = [h.detach() for h in hidden]\n",
    "            \n",
    "            # 6. Modified loss calculation with mixup\n",
    "            ce_loss = lam * criterion(logits, target_a) + (1 - lam) * criterion(logits, target_b)\n",
    "            recon_loss = recon_criterion(recon, inputs)\n",
    "            loss = ce_loss + 0.05 * recon_loss  # Reduced reconstruction weight\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Reduced from 1.0\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (lam * predicted.eq(target_a).float() + (1 - lam) * predicted.eq(target_b).float()).sum().item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        hidden = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # 7. Test-time augmentation\n",
    "                batch_size = inputs.size(0)\n",
    "                tta_samples = 3\n",
    "                tta_predictions = torch.zeros(batch_size, model.classifier[-1].out_features).to(device)\n",
    "                \n",
    "                for _ in range(tta_samples):\n",
    "                    logits, _, _ = model(inputs, hidden)\n",
    "                    tta_predictions += F.softmax(logits, dim=1)\n",
    "                \n",
    "                tta_predictions /= tta_samples\n",
    "                \n",
    "                loss = criterion(tta_predictions, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = tta_predictions.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Update learning rate based on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch}')\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # 8. Return best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, labels, augment=False):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, eeg in enumerate(tqdm(data, desc=\"Creating spectrograms\")):\n",
    "            for channel in range(eeg.shape[0]):\n",
    "                spectrogram = create_spectrogram(preprocess_eeg(eeg[channel]))\n",
    "                if augment:\n",
    "                    specs = [\n",
    "                        spectrogram,\n",
    "                        jitter(spectrogram, sigma=0.01),\n",
    "                        scaling(spectrogram, sigma=0.1),\n",
    "                        add_noise(spectrogram, noise_level=0.01),\n",
    "                        time_warp(spectrogram, sigma=2)\n",
    "                    ]\n",
    "                    for spec in specs:\n",
    "                        self.data.append(spec)\n",
    "                        self.labels.append(labels[i])\n",
    "                else:\n",
    "                    self.data.append(spectrogram)\n",
    "                    self.labels.append(labels[i])\n",
    "        \n",
    "        self.data = torch.FloatTensor(np.stack(self.data))\n",
    "        self.data = self.data.unsqueeze(1)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def add_noise(x, noise_level=0.01):\n",
    "    return x + np.random.randn(*x.shape) * noise_level\n",
    "\n",
    "def time_warp(x, sigma=2):\n",
    "    \"\"\"Apply time warping to a 2D spectrogram.\"\"\"\n",
    "    x = np.array(x)\n",
    "    if len(x.shape) != 2:\n",
    "        raise ValueError(f\"Expected 2D input, got shape {x.shape}\")\n",
    "        \n",
    "    time_steps = np.arange(x.shape[1])\n",
    "    # Generate random warping\n",
    "    warp = np.random.normal(loc=1.0, scale=sigma, size=(1,))\n",
    "    warp_steps = time_steps * warp\n",
    "    \n",
    "    # Ensure warp_steps are within bounds\n",
    "    warp_steps = np.clip(warp_steps, 0, x.shape[1]-1)\n",
    "    \n",
    "    # Apply warping to each frequency bin\n",
    "    warped = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        warped[i, :] = np.interp(time_steps, warp_steps, x[i, :])\n",
    "    \n",
    "    return warped\n",
    "\n",
    "def jitter(x, sigma=0.01):\n",
    "    \"\"\"Add random jitter noise.\"\"\"\n",
    "    return x + np.random.normal(0, sigma, x.shape)\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    \"\"\"Apply random scaling.\"\"\"\n",
    "    factor = np.random.normal(1.0, sigma)\n",
    "    return x * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 6, 2500) [3 4 5 6 7 8 9]\n",
      "(36, 6, 2500) [0 1 2]\n",
      "Class shape: (120,)\n",
      "Data shape: (120, 6, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating spectrograms: 100%|██████████| 120/120 [00:01<00:00, 113.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_eeg_data(file_path):\n",
    "    \"\"\"Load EEG data from OpenBCI-RAW files\"\"\"\n",
    "    try:\n",
    "        # Skip header rows and load only first 6 EEG channels\n",
    "        data = pd.read_csv(file_path, skiprows=6)\n",
    "        eeg_data = data.iloc[:, 1:7].values\n",
    "        return eeg_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return None\n",
    "def load_folder_data(root_path, tasks=['baseline', 'counting', 'logical', 'rotation']):\n",
    "    root = Path(root_path)\n",
    "    X, y = [], []\n",
    "    subjects = [d for d in root.iterdir() if d.is_dir() and d.name in ['eshwa', 'Giri', 'Nithish']]\n",
    "    \n",
    "    for subject_idx, subject in enumerate(subjects):\n",
    "        for task in tasks:\n",
    "            session_pattern = f\"OpenBCISession_{subject.name.lower()}_{task}_*\"\n",
    "            sessions = list(subject.glob(session_pattern))\n",
    "            \n",
    "            for session in sessions:\n",
    "                raw_files = list(session.glob(\"OpenBCI-RAW-*.txt\"))\n",
    "                \n",
    "                for raw_file in raw_files:\n",
    "                    eeg_data = load_eeg_data(raw_file)\n",
    "                    if eeg_data is not None:\n",
    "                        # Ensure we have enough data to slice 2500 samples\n",
    "                        if eeg_data.shape[0] >= 6250:\n",
    "                            # Extract 2500 samples from each of the 6 channels\n",
    "                            X.append(eeg_data[3750:6250, :6])\n",
    "                            y.append(subject_idx)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Check if we have the desired number of samples\n",
    "    if X.shape[0] != 36:\n",
    "        print(f\"Warning: Expected 36 samples, but got {X.shape[0]}\")\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "mat_inputs, mat_targets = load_mat_data('eegdata.mat')\n",
    "folder_inputs, folder_targets = load_folder_data('dataset')\n",
    "\n",
    "# Ensure both datasets have the same number of dimensions\n",
    "if mat_inputs.ndim == 3 and folder_inputs.ndim == 2:\n",
    "    folder_inputs = folder_inputs[:, np.newaxis, :]  # Add a new axis to match dimensions\n",
    "\n",
    "print(mat_inputs.shape,np.unique(mat_targets))\n",
    "\n",
    "folder_inputs = folder_inputs.transpose(0, 2, 1)\n",
    "print(folder_inputs.shape,np.unique(folder_targets))\n",
    "X = np.concatenate((folder_inputs,mat_inputs), axis=0)\n",
    "y = np.concatenate((folder_targets,mat_targets), axis=0)\n",
    "\n",
    "# Print class and data shapes\n",
    "print(f\"Class shape: {y.shape}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = EEGDataset(X, y, augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(create_spectrogram(X[0][0]), aspect='auto', cmap='viridis')\n",
    "plt.title('Sample Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(y[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataset creation\n",
    "dataset = EEGDataset(X, y, augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Use smaller batch size for CPU\n",
    "num_classes = len(np.unique(y))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 90/90 [00:15<00:00,  5.92it/s, loss=0.0713, acc=15.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 2.2816, Train Acc: 15.87%\n",
      "Val Loss: 2.1164, Val Acc: 21.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 90/90 [00:14<00:00,  6.11it/s, loss=0.0666, acc=21.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 2.1320, Train Acc: 21.60%\n",
      "Val Loss: 2.0027, Val Acc: 22.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 90/90 [00:14<00:00,  6.06it/s, loss=0.0636, acc=23.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 2.0355, Train Acc: 23.61%\n",
      "Val Loss: 1.9254, Val Acc: 27.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 90/90 [00:15<00:00,  5.86it/s, loss=0.0613, acc=27.29%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 1.9630, Train Acc: 27.29%\n",
      "Val Loss: 1.8701, Val Acc: 32.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50:   1%|          | 1/90 [00:00<00:28,  3.11it/s, loss=0.0618, acc=18.75%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 234\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Initialize and train model\u001b[39;00m\n\u001b[0;32m    228\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGClassifier(\n\u001b[0;32m    229\u001b[0m     input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    230\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[0;32m    231\u001b[0m     dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m  \u001b[38;5;66;03m# Reduced dropout\u001b[39;00m\n\u001b[0;32m    232\u001b[0m )\n\u001b[1;32m--> 234\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced learning rate\u001b[39;49;00m\n\u001b[0;32m    240\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m recon_loss\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Update dataset creation\n",
    "dataset = EEGDataset(X, y, augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Use smaller batch size for CPU\n",
    "num_classes = len(np.unique(y))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# Train with optimized parameters\n",
    "model = EEGClassifier(\n",
    "    input_channels=1,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=0.5\n",
    ")\n",
    "\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TTTCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, eta=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Hidden state model (f in the paper)\n",
    "        self.hidden_model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize cache for test-time updates\n",
    "        self.cache_params = {}\n",
    "        \n",
    "    def forward(self, x, hidden, is_training=True):\n",
    "        if is_training:\n",
    "            next_hidden = self.hidden_model(x)\n",
    "        else:\n",
    "            # Test-time training update\n",
    "            with torch.enable_grad():\n",
    "                x_temp = x.detach().requires_grad_()\n",
    "                pred = self.hidden_model(x_temp)\n",
    "                \n",
    "                # Self-supervised loss (as described in paper section 2.1)\n",
    "                l2_loss = pred.pow(2).mean()\n",
    "                consistency_loss = torch.abs(pred - x_temp.mean(dim=-1, keepdim=True)).mean()\n",
    "                loss = l2_loss + 0.1 * consistency_loss\n",
    "                \n",
    "                # Update hidden model parameters\n",
    "                grads = torch.autograd.grad(loss, self.hidden_model.parameters())\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for param, grad in zip(self.hidden_model.parameters(), grads):\n",
    "                        if f'momentum_{id(param)}' not in self.cache_params:\n",
    "                            self.cache_params[f'momentum_{id(param)}'] = torch.zeros_like(param)\n",
    "                        \n",
    "                        # Momentum update\n",
    "                        self.cache_params[f'momentum_{id(param)}'].mul_(0.9).add_(grad, alpha=0.1)\n",
    "                        param.add_(self.cache_params[f'momentum_{id(param)}'], alpha=-self.eta)\n",
    "                \n",
    "                next_hidden = self.hidden_model(x)\n",
    "                \n",
    "        return next_hidden\n",
    "\n",
    "\n",
    "class TTTRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create TTT cells for each layer\n",
    "        self.cells = nn.ModuleList([\n",
    "            TTTCell(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                hidden_size\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, hidden=None, is_training=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = [torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                     for _ in range(self.num_layers)]\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            layer_input = x[:, t, :]\n",
    "            for layer_idx, cell in enumerate(self.cells):\n",
    "                hidden[layer_idx] = cell(layer_input, hidden[layer_idx], is_training)\n",
    "                layer_input = hidden[layer_idx]\n",
    "            outputs.append(layer_input)\n",
    "            \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=10, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.cache_params = {}\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Initial block\n",
    "        #     nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2),  # 25x25\n",
    "        #     nn.Dropout(0.2),\n",
    "            \n",
    "        #     # Second block\n",
    "        #     nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2),  # 12x12\n",
    "        #     nn.Dropout(0.2),\n",
    "            \n",
    "        #     # Third block\n",
    "        #     nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2),  # 6x6\n",
    "        #     nn.Dropout(0.2),\n",
    "            \n",
    "        #     # Fourth block with global pooling\n",
    "        #     nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling to 1x1\n",
    "        # )\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),  # Added dropout\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # TTT blocks\n",
    "        self.ttt_blocks = nn.ModuleList([\n",
    "            TTTEEGBlock(hidden_size=256, num_heads=4)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),  # Now using 256 features from global pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Reconstruction decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 1024),  # From 256 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 50 * 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Print input shape for debugging\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        # Ensure input shape [B, C, H, W]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)  # [B, 256, 1, 1]\n",
    "        # print(\"After feature extractor:\", features.shape)\n",
    "        \n",
    "        # Reshape for TTT blocks\n",
    "        features = features.squeeze(-1).squeeze(-1)  # [B, 256]\n",
    "        features = features.unsqueeze(1)  # [B, 1, 256]\n",
    "        \n",
    "        # Apply TTT blocks\n",
    "        for block in self.ttt_blocks:\n",
    "            features = block(features, self.cache_params)\n",
    "        \n",
    "        # Prepare for classification\n",
    "        features = features.squeeze(1)  # [B, 256]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon = self.decoder(features)\n",
    "        recon = recon.view(-1, 1, 50, 50)\n",
    "        \n",
    "        return logits, recon\n",
    "    \n",
    "def SEBlock(channels):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Conv2d(channels, channels, kernel_size=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "def ResidualBlock(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.GELU(),\n",
    "        SEBlock(out_channels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 90/90 [00:21<00:00,  4.23it/s, loss=0.0604, acc=34.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 1.9330, Train Acc: 34.51%\n",
      "Val Loss: 1.7204, Val Acc: 45.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 90/90 [00:20<00:00,  4.30it/s, loss=0.0493, acc=54.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 1.5784, Train Acc: 54.55%\n",
      "Val Loss: 1.4615, Val Acc: 58.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 90/90 [00:20<00:00,  4.34it/s, loss=0.0409, acc=69.27%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 1.3081, Train Acc: 69.27%\n",
      "Val Loss: 1.2093, Val Acc: 76.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 90/90 [00:20<00:00,  4.30it/s, loss=0.0345, acc=77.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 1.1054, Train Acc: 77.40%\n",
      "Val Loss: 1.1440, Val Acc: 71.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 90/90 [00:20<00:00,  4.34it/s, loss=0.0291, acc=84.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "Train Loss: 0.9327, Train Acc: 84.76%\n",
      "Val Loss: 0.9877, Val Acc: 82.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 90/90 [00:20<00:00,  4.32it/s, loss=0.0251, acc=90.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "Train Loss: 0.8020, Train Acc: 90.35%\n",
      "Val Loss: 0.9572, Val Acc: 84.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 90/90 [00:21<00:00,  4.20it/s, loss=0.0229, acc=92.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "Train Loss: 0.7331, Train Acc: 92.57%\n",
      "Val Loss: 1.2297, Val Acc: 63.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 90/90 [00:21<00:00,  4.24it/s, loss=0.0218, acc=93.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "Train Loss: 0.6976, Train Acc: 93.82%\n",
      "Val Loss: 1.0706, Val Acc: 74.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 90/90 [00:21<00:00,  4.26it/s, loss=0.0209, acc=94.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "Train Loss: 0.6689, Train Acc: 94.90%\n",
      "Val Loss: 1.3997, Val Acc: 58.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 90/90 [00:21<00:00,  4.23it/s, loss=0.0206, acc=95.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "Train Loss: 0.6583, Train Acc: 95.31%\n",
      "Val Loss: 0.9281, Val Acc: 87.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 90/90 [00:21<00:00,  4.20it/s, loss=0.0200, acc=96.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "Train Loss: 0.6388, Train Acc: 96.01%\n",
      "Val Loss: 1.3589, Val Acc: 61.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 90/90 [00:21<00:00,  4.17it/s, loss=0.0194, acc=97.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "Train Loss: 0.6222, Train Acc: 97.08%\n",
      "Val Loss: 1.4533, Val Acc: 56.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 90/90 [00:21<00:00,  4.16it/s, loss=0.0190, acc=97.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "Train Loss: 0.6081, Train Acc: 97.43%\n",
      "Val Loss: 1.1401, Val Acc: 75.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 90/90 [00:22<00:00,  4.03it/s, loss=0.0187, acc=97.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "Train Loss: 0.5980, Train Acc: 97.95%\n",
      "Val Loss: 1.3772, Val Acc: 62.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 90/90 [00:23<00:00,  3.83it/s, loss=0.0181, acc=98.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "Train Loss: 0.5808, Train Acc: 98.51%\n",
      "Val Loss: 1.5295, Val Acc: 62.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 90/90 [00:23<00:00,  3.79it/s, loss=0.0177, acc=98.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "Train Loss: 0.5654, Train Acc: 98.78%\n",
      "Val Loss: 1.0335, Val Acc: 81.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 90/90 [00:22<00:00,  3.94it/s, loss=0.0175, acc=99.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "Train Loss: 0.5592, Train Acc: 99.31%\n",
      "Val Loss: 1.3159, Val Acc: 64.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 90/90 [00:23<00:00,  3.90it/s, loss=0.0174, acc=99.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "Train Loss: 0.5576, Train Acc: 99.13%\n",
      "Val Loss: 0.8448, Val Acc: 90.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 90/90 [00:24<00:00,  3.69it/s, loss=0.0171, acc=99.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "Train Loss: 0.5459, Train Acc: 99.58%\n",
      "Val Loss: 1.1060, Val Acc: 74.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 90/90 [00:24<00:00,  3.70it/s, loss=0.0167, acc=99.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "Train Loss: 0.5344, Train Acc: 99.86%\n",
      "Val Loss: 0.8542, Val Acc: 88.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 90/90 [00:23<00:00,  3.83it/s, loss=0.0168, acc=99.62%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "Train Loss: 0.5383, Train Acc: 99.62%\n",
      "Val Loss: 0.8157, Val Acc: 90.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 90/90 [00:23<00:00,  3.84it/s, loss=0.0166, acc=99.83%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "Train Loss: 0.5307, Train Acc: 99.83%\n",
      "Val Loss: 0.9987, Val Acc: 79.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 90/90 [00:23<00:00,  3.84it/s, loss=0.0165, acc=99.72%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "Train Loss: 0.5289, Train Acc: 99.72%\n",
      "Val Loss: 0.8207, Val Acc: 89.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 90/90 [00:24<00:00,  3.72it/s, loss=0.0164, acc=99.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "Train Loss: 0.5239, Train Acc: 99.93%\n",
      "Val Loss: 0.9683, Val Acc: 79.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 90/90 [00:25<00:00,  3.57it/s, loss=0.0162, acc=99.97%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "Train Loss: 0.5192, Train Acc: 99.97%\n",
      "Val Loss: 0.7264, Val Acc: 91.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 90/90 [00:26<00:00,  3.44it/s, loss=0.0162, acc=99.93%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50:\n",
      "Train Loss: 0.5179, Train Acc: 99.93%\n",
      "Val Loss: 0.7609, Val Acc: 90.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50:  47%|████▋     | 42/90 [00:12<00:14,  3.35it/s, loss=0.0162, acc=99.93%] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGClassifier(\n\u001b[0;32m      2\u001b[0m     input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      3\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[0;32m      4\u001b[0m     dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m trained_model, train_losses, val_losses, train_accs, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m recon_loss\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EEGClassifier(\n",
    "    input_channels=1,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "trained_model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 90/90 [00:33<00:00,  2.67it/s, loss=2.0868, acc=30.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 2.0868, Train Acc: 30.31%\n",
      "Val Loss: 2.0468, Val Acc: 35.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 90/90 [00:35<00:00,  2.55it/s, loss=1.8928, acc=42.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 1.8928, Train Acc: 42.85%\n",
      "Val Loss: 1.9941, Val Acc: 33.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 90/90 [00:34<00:00,  2.63it/s, loss=1.7796, acc=51.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 1.7796, Train Acc: 51.60%\n",
      "Val Loss: 1.9610, Val Acc: 31.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 90/90 [00:33<00:00,  2.67it/s, loss=1.7162, acc=56.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 1.7162, Train Acc: 56.01%\n",
      "Val Loss: 1.9582, Val Acc: 28.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 90/90 [00:34<00:00,  2.62it/s, loss=1.6014, acc=65.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "Train Loss: 1.6014, Train Acc: 65.28%\n",
      "Val Loss: 1.9754, Val Acc: 30.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 90/90 [00:36<00:00,  2.48it/s, loss=1.5403, acc=69.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "Train Loss: 1.5403, Train Acc: 69.06%\n",
      "Val Loss: 1.9364, Val Acc: 33.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 90/90 [00:35<00:00,  2.56it/s, loss=1.5117, acc=71.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "Train Loss: 1.5117, Train Acc: 71.39%\n",
      "Val Loss: 1.9031, Val Acc: 40.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 90/90 [00:36<00:00,  2.45it/s, loss=1.4569, acc=74.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "Train Loss: 1.4569, Train Acc: 74.13%\n",
      "Val Loss: 1.9155, Val Acc: 40.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 90/90 [00:36<00:00,  2.50it/s, loss=1.3854, acc=77.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "Train Loss: 1.3854, Train Acc: 77.57%\n",
      "Val Loss: 1.9264, Val Acc: 42.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 90/90 [00:38<00:00,  2.34it/s, loss=1.3325, acc=80.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "Train Loss: 1.3325, Train Acc: 80.97%\n",
      "Val Loss: 1.8832, Val Acc: 50.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 90/90 [00:35<00:00,  2.52it/s, loss=1.2872, acc=84.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "Train Loss: 1.2872, Train Acc: 84.06%\n",
      "Val Loss: 1.8701, Val Acc: 45.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 90/90 [00:35<00:00,  2.50it/s, loss=1.2744, acc=84.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "Train Loss: 1.2744, Train Acc: 84.17%\n",
      "Val Loss: 1.8438, Val Acc: 51.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 90/90 [00:38<00:00,  2.35it/s, loss=1.2551, acc=85.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "Train Loss: 1.2551, Train Acc: 85.66%\n",
      "Val Loss: 1.8197, Val Acc: 55.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 90/90 [00:38<00:00,  2.33it/s, loss=1.2438, acc=86.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "Train Loss: 1.2438, Train Acc: 86.25%\n",
      "Val Loss: 1.7764, Val Acc: 57.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 90/90 [00:36<00:00,  2.43it/s, loss=1.2382, acc=86.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "Train Loss: 1.2382, Train Acc: 86.53%\n",
      "Val Loss: 1.7913, Val Acc: 52.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 90/90 [00:36<00:00,  2.45it/s, loss=1.2000, acc=87.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "Train Loss: 1.2000, Train Acc: 87.99%\n",
      "Val Loss: 1.7763, Val Acc: 57.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 90/90 [00:36<00:00,  2.44it/s, loss=1.1617, acc=89.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "Train Loss: 1.1617, Train Acc: 89.86%\n",
      "Val Loss: 1.7906, Val Acc: 52.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 90/90 [00:37<00:00,  2.43it/s, loss=1.1306, acc=90.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "Train Loss: 1.1306, Train Acc: 90.83%\n",
      "Val Loss: 1.8061, Val Acc: 49.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 90/90 [00:37<00:00,  2.37it/s, loss=1.1048, acc=92.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "Train Loss: 1.1048, Train Acc: 92.57%\n",
      "Val Loss: 1.7859, Val Acc: 51.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 90/90 [00:38<00:00,  2.31it/s, loss=1.0851, acc=93.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "Train Loss: 1.0851, Train Acc: 93.61%\n",
      "Val Loss: 1.7645, Val Acc: 55.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 90/90 [00:40<00:00,  2.23it/s, loss=1.0706, acc=93.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "Train Loss: 1.0706, Train Acc: 93.68%\n",
      "Val Loss: 1.7615, Val Acc: 55.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 90/90 [00:39<00:00,  2.29it/s, loss=1.0530, acc=94.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "Train Loss: 1.0530, Train Acc: 94.62%\n",
      "Val Loss: 1.7432, Val Acc: 53.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 90/90 [00:40<00:00,  2.25it/s, loss=1.0455, acc=95.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "Train Loss: 1.0455, Train Acc: 95.35%\n",
      "Val Loss: 1.7372, Val Acc: 55.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 90/90 [00:40<00:00,  2.22it/s, loss=1.0380, acc=94.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "Train Loss: 1.0380, Train Acc: 94.93%\n",
      "Val Loss: 1.7057, Val Acc: 59.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 90/90 [00:40<00:00,  2.23it/s, loss=1.0305, acc=95.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "Train Loss: 1.0305, Train Acc: 95.80%\n",
      "Val Loss: 1.7009, Val Acc: 59.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 90/90 [00:39<00:00,  2.27it/s, loss=1.0265, acc=95.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50:\n",
      "Train Loss: 1.0265, Train Acc: 95.52%\n",
      "Val Loss: 1.6533, Val Acc: 64.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 90/90 [00:40<00:00,  2.24it/s, loss=1.0253, acc=95.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50:\n",
      "Train Loss: 1.0253, Train Acc: 95.62%\n",
      "Val Loss: 1.6428, Val Acc: 64.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 90/90 [00:40<00:00,  2.23it/s, loss=1.0235, acc=96.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50:\n",
      "Train Loss: 1.0235, Train Acc: 96.04%\n",
      "Val Loss: 1.6196, Val Acc: 65.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 90/90 [00:40<00:00,  2.20it/s, loss=1.0242, acc=95.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50:\n",
      "Train Loss: 1.0242, Train Acc: 95.80%\n",
      "Val Loss: 1.5963, Val Acc: 70.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 90/90 [00:40<00:00,  2.20it/s, loss=1.0212, acc=95.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50:\n",
      "Train Loss: 1.0212, Train Acc: 95.83%\n",
      "Val Loss: 1.6165, Val Acc: 65.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 90/90 [00:43<00:00,  2.06it/s, loss=1.0149, acc=95.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50:\n",
      "Train Loss: 1.0149, Train Acc: 95.97%\n",
      "Val Loss: 1.6545, Val Acc: 62.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s, loss=1.0006, acc=96.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50:\n",
      "Train Loss: 1.0006, Train Acc: 96.60%\n",
      "Val Loss: 1.5518, Val Acc: 80.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 90/90 [00:43<00:00,  2.08it/s, loss=0.9904, acc=97.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50:\n",
      "Train Loss: 0.9904, Train Acc: 97.01%\n",
      "Val Loss: 1.6021, Val Acc: 69.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 90/90 [00:42<00:00,  2.11it/s, loss=0.9830, acc=97.22%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 229\u001b[0m\n\u001b[0;32m    219\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    220\u001b[0m     val_dataset,\n\u001b[0;32m    221\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m trained_model, train_losses, val_losses, train_accs, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m    234\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 119\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    116\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_augmentations):\n\u001b[1;32m--> 119\u001b[0m     logits, recon \u001b[38;5;241m=\u001b[39m \u001b[43mema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Average predictions\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 150\u001b[0m, in \u001b[0;36mEEGClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, 256, 1, 1]\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# print(\"After feature extractor:\", features.shape)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Reshape for TTT blocks\u001b[39;00m\n\u001b[0;32m    154\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 256]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer with conservative parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.00005,  # Very conservative learning rate\n",
    "        weight_decay=0.1,  # Strong regularization\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing scheduler with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  # Initial cycle length\n",
    "        T_mult=2,  # Cycle length multiplier\n",
    "        eta_min=1e-6  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    # Loss functions with label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize EMA model\n",
    "    ema = torch.optim.swa_utils.AveragedModel(model)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_acc = 0\n",
    "    patience = 15  # Increased patience\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Number of gradient accumulation steps\n",
    "    num_accumulation_steps = 4\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero gradients at the start of accumulation\n",
    "            if batch_idx % num_accumulation_steps == 0:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, recon = model(inputs)\n",
    "            \n",
    "            # Calculate losses\n",
    "            ce_loss = criterion(logits, targets)\n",
    "            recon_loss = recon_criterion(recon, inputs)\n",
    "            loss = ce_loss + 0.05 * recon_loss  # Reduced reconstruction weight\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / num_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights after accumulation steps\n",
    "            if (batch_idx + 1) % num_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update EMA model\n",
    "                ema.update_parameters(model)\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * num_accumulation_steps\n",
    "            _, predicted = logits.max(1)\n",
    "            running_total += targets.size(0)\n",
    "            running_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            current_acc = 100. * running_correct / running_total\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * running_correct / running_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Use EMA model for validation\n",
    "            ema.eval()\n",
    "            \n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device).float()\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # Test-time augmentation\n",
    "                n_augmentations = 5\n",
    "                predictions = []\n",
    "                \n",
    "                for _ in range(n_augmentations):\n",
    "                    logits, recon = ema.module(inputs)\n",
    "                    predictions.append(F.softmax(logits, dim=1))\n",
    "                \n",
    "                # Average predictions\n",
    "                averaged_preds = torch.stack(predictions).mean(0)\n",
    "                \n",
    "                # Calculate loss with averaged predictions\n",
    "                loss = criterion(averaged_preds.log(), targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = averaged_preds.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = ema.module.state_dict().copy()  # Save EMA model state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    # Plot smooth training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    def smooth_curve(points, factor=0.8):\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                previous = smoothed_points[-1]\n",
    "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(smooth_curve(train_losses), label='Training Loss')\n",
    "    plt.plot(smooth_curve(val_losses), label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(smooth_curve(train_accs), label='Training Accuracy')\n",
    "    plt.plot(smooth_curve(val_accs), label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Usage example:\n",
    "model = EEGClassifier(\n",
    "    input_channels=1,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# Create data loaders with consistent batch size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trained_model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TTTEEGBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4, head_dim=None, eta=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim if head_dim is not None else hidden_size // num_heads\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Initialize with smaller values for stability\n",
    "        self.W1 = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim) * 0.01)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_heads, 1, self.head_dim))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Add skip connection scaling\n",
    "        self.skip_scale = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, x, cache_params=None):\n",
    "        B, L, C = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        x = x.reshape(B, L, self.num_heads, self.head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)  # [B, H, L, D]\n",
    "        \n",
    "        if cache_params is None:\n",
    "            output = self.dropout(torch.einsum('bhld,hdf->bhlf', x, self.W1) + self.b1.unsqueeze(0))\n",
    "        else:\n",
    "            with torch.enable_grad():\n",
    "                x_temp = x.detach().requires_grad_()\n",
    "                output_temp = torch.einsum('bhld,hdf->bhlf', x_temp, self.W1) + self.b1.unsqueeze(0)\n",
    "                \n",
    "                # Modified self-supervised loss\n",
    "                l2_loss = 0.5 * output_temp.pow(2).mean()\n",
    "                consistency_loss = F.smooth_l1_loss(\n",
    "                    output_temp, \n",
    "                    x_temp.mean(dim=-1, keepdim=True),\n",
    "                    beta=0.1\n",
    "                )\n",
    "                self_supervised_loss = l2_loss + 0.05 * consistency_loss\n",
    "                \n",
    "                # Compute and stabilize gradients\n",
    "                grads = torch.autograd.grad(self_supervised_loss, [self.W1, self.b1], \n",
    "                                         retain_graph=True, create_graph=False)\n",
    "                grad_W1, grad_b1 = grads\n",
    "                \n",
    "                # Gradient clipping with dynamic threshold\n",
    "                max_norm = 0.1 * (1.0 - math.exp(-len(cache_params) / 100))\n",
    "                grad_W1 = torch.clamp(grad_W1, -max_norm, max_norm)\n",
    "                grad_b1 = torch.clamp(grad_b1, -max_norm, max_norm)\n",
    "                \n",
    "                if f'W1_states_{id(self)}' not in cache_params:\n",
    "                    cache_params[f'W1_states_{id(self)}'] = self.W1.clone()\n",
    "                    cache_params[f'b1_states_{id(self)}'] = self.b1.clone()\n",
    "                    cache_params[f'momentum_W1_{id(self)}'] = torch.zeros_like(self.W1)\n",
    "                    cache_params[f'momentum_b1_{id(self)}'] = torch.zeros_like(self.b1)\n",
    "                \n",
    "                # Adaptive momentum\n",
    "                beta1 = min(0.9, 1.0 - math.exp(-len(cache_params) / 50))\n",
    "                cache_params[f'momentum_W1_{id(self)}'] = (\n",
    "                    beta1 * cache_params[f'momentum_W1_{id(self)}'] + \n",
    "                    (1 - beta1) * grad_W1\n",
    "                )\n",
    "                cache_params[f'momentum_b1_{id(self)}'] = (\n",
    "                    beta1 * cache_params[f'momentum_b1_{id(self)}'] + \n",
    "                    (1 - beta1) * grad_b1\n",
    "                )\n",
    "                \n",
    "                # Adaptive learning rate\n",
    "                lr_scale = math.exp(-len(cache_params) / 200)\n",
    "                cache_params[f'W1_states_{id(self)}'] -= self.eta * lr_scale * cache_params[f'momentum_W1_{id(self)}']\n",
    "                cache_params[f'b1_states_{id(self)}'] -= self.eta * lr_scale * cache_params[f'momentum_b1_{id(self)}']\n",
    "                \n",
    "                output = self.dropout(torch.einsum('bhld,hdf->bhlf', x, \n",
    "                                    cache_params[f'W1_states_{id(self)}']) + \n",
    "                                    cache_params[f'b1_states_{id(self)}'].unsqueeze(0))\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3)  # [B, L, H, D]\n",
    "        output = output.reshape(B, L, -1)     # [B, L, H*D]\n",
    "        \n",
    "        # Scaled skip connection\n",
    "        return output + self.skip_scale * residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eshwa\\AppData\\Local\\Temp\\ipykernel_24264\\1028220634.py:37: UserWarning: Using a target size (torch.Size([32, 4, 1, 1])) that is different to the input size (torch.Size([32, 4, 1, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  consistency_loss = F.smooth_l1_loss(\n",
      "Epoch 1/50: 100%|██████████| 90/90 [00:34<00:00,  2.61it/s, loss=2.2575, acc=20.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 2.2575, Train Acc: 20.56%\n",
      "Val Loss: 2.2315, Val Acc: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 90/90 [00:33<00:00,  2.68it/s, loss=2.1754, acc=28.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 2.1754, Train Acc: 28.65%\n",
      "Val Loss: 2.2114, Val Acc: 23.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 90/90 [00:33<00:00,  2.67it/s, loss=2.0979, acc=32.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 2.0979, Train Acc: 32.12%\n",
      "Val Loss: 2.1846, Val Acc: 26.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 90/90 [00:33<00:00,  2.67it/s, loss=2.0493, acc=34.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 2.0493, Train Acc: 34.76%\n",
      "Val Loss: 2.1699, Val Acc: 27.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 90/90 [00:34<00:00,  2.64it/s, loss=1.9746, acc=39.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "Train Loss: 1.9746, Train Acc: 39.55%\n",
      "Val Loss: 2.1418, Val Acc: 29.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 90/90 [00:34<00:00,  2.60it/s, loss=1.9362, acc=43.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "Train Loss: 1.9362, Train Acc: 43.23%\n",
      "Val Loss: 2.1122, Val Acc: 29.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 90/90 [00:34<00:00,  2.58it/s, loss=1.9261, acc=43.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "Train Loss: 1.9261, Train Acc: 43.85%\n",
      "Val Loss: 2.0914, Val Acc: 30.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 90/90 [00:41<00:00,  2.18it/s, loss=1.8825, acc=47.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "Train Loss: 1.8825, Train Acc: 47.36%\n",
      "Val Loss: 2.0805, Val Acc: 23.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 90/90 [00:39<00:00,  2.28it/s, loss=1.8243, acc=51.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "Train Loss: 1.8243, Train Acc: 51.81%\n",
      "Val Loss: 2.0650, Val Acc: 21.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 90/90 [00:38<00:00,  2.36it/s, loss=1.7759, acc=55.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "Train Loss: 1.7759, Train Acc: 55.38%\n",
      "Val Loss: 2.0580, Val Acc: 20.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 90/90 [00:37<00:00,  2.38it/s, loss=1.7370, acc=56.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "Train Loss: 1.7370, Train Acc: 56.63%\n",
      "Val Loss: 2.0385, Val Acc: 20.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 90/90 [00:39<00:00,  2.28it/s, loss=1.7126, acc=59.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "Train Loss: 1.7126, Train Acc: 59.10%\n",
      "Val Loss: 2.0233, Val Acc: 21.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 90/90 [00:39<00:00,  2.28it/s, loss=1.6987, acc=60.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "Train Loss: 1.6987, Train Acc: 60.45%\n",
      "Val Loss: 2.0096, Val Acc: 22.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 90/90 [00:37<00:00,  2.42it/s, loss=1.6940, acc=59.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "Train Loss: 1.6940, Train Acc: 59.72%\n",
      "Val Loss: 1.9907, Val Acc: 24.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 90/90 [00:37<00:00,  2.40it/s, loss=1.6811, acc=58.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "Train Loss: 1.6811, Train Acc: 58.96%\n",
      "Val Loss: 1.9575, Val Acc: 29.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 90/90 [00:37<00:00,  2.38it/s, loss=1.6319, acc=61.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "Train Loss: 1.6319, Train Acc: 61.84%\n",
      "Val Loss: 1.9619, Val Acc: 27.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 90/90 [00:38<00:00,  2.35it/s, loss=1.5932, acc=63.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "Train Loss: 1.5932, Train Acc: 63.44%\n",
      "Val Loss: 1.9424, Val Acc: 30.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 90/90 [00:37<00:00,  2.37it/s, loss=1.5499, acc=66.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "Train Loss: 1.5499, Train Acc: 66.77%\n",
      "Val Loss: 1.9475, Val Acc: 30.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 90/90 [00:39<00:00,  2.31it/s, loss=1.5201, acc=68.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "Train Loss: 1.5201, Train Acc: 68.99%\n",
      "Val Loss: 1.9293, Val Acc: 33.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 90/90 [00:38<00:00,  2.33it/s, loss=1.4912, acc=71.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "Train Loss: 1.4912, Train Acc: 71.88%\n",
      "Val Loss: 1.8943, Val Acc: 39.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 90/90 [00:39<00:00,  2.27it/s, loss=1.4701, acc=71.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "Train Loss: 1.4701, Train Acc: 71.56%\n",
      "Val Loss: 1.8690, Val Acc: 41.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 90/90 [00:39<00:00,  2.28it/s, loss=1.4519, acc=74.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "Train Loss: 1.4519, Train Acc: 74.38%\n",
      "Val Loss: 1.8534, Val Acc: 41.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 90/90 [00:40<00:00,  2.24it/s, loss=1.4435, acc=75.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "Train Loss: 1.4435, Train Acc: 75.14%\n",
      "Val Loss: 1.8470, Val Acc: 42.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 90/90 [00:38<00:00,  2.32it/s, loss=1.4273, acc=75.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "Train Loss: 1.4273, Train Acc: 75.97%\n",
      "Val Loss: 1.8355, Val Acc: 42.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 90/90 [00:40<00:00,  2.22it/s, loss=1.4211, acc=76.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "Train Loss: 1.4211, Train Acc: 76.15%\n",
      "Val Loss: 1.8127, Val Acc: 46.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50:  44%|████▍     | 40/90 [00:18<00:23,  2.14it/s, loss=1.4323, acc=74.69%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 28\u001b[0m\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     19\u001b[0m     val_dataset,\n\u001b[0;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m trained_model, train_losses, val_losses, train_accs, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 70\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Scale loss for gradient accumulation\u001b[39;00m\n\u001b[0;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_accumulation_steps\n\u001b[1;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Update weights after accumulation steps\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m num_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model (keeping your original architecture)\n",
    "model = EEGClassifier(\n",
    "    input_channels=1,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# Train with smaller batch size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trained_model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 1. Stable optimizer with very conservative settings\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,  # Ultra-low learning rate for stability\n",
    "        weight_decay=0.01,  # Moderate weight decay\n",
    "        betas=(0.9, 0.999),  # Higher beta2 for momentum stability\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # 2. Gradual warm-up scheduler\n",
    "    def lr_lambda(epoch):\n",
    "        warmup_epochs = 5\n",
    "        if epoch < warmup_epochs:\n",
    "            return epoch / warmup_epochs\n",
    "        return 0.5 ** ((epoch - warmup_epochs) // 10)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # 3. Loss functions with stability focus\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # Reduced smoothing\n",
    "    recon_criterion = nn.SmoothL1Loss(beta=0.5)  # Higher beta for stability\n",
    "    \n",
    "    # 4. Initialize validation tracking\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    validation_window = []  # For smoothed validation accuracy\n",
    "    window_size = 3\n",
    "    \n",
    "    # 5. Training metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, recon = model(inputs)\n",
    "            \n",
    "            # Calculate losses with stability focus\n",
    "            ce_loss = criterion(logits, targets)\n",
    "            recon_loss = recon_criterion(recon, inputs)\n",
    "            loss = ce_loss + 0.01 * recon_loss  # Reduced reconstruction weight\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # Conservative clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase with stability measures\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Multiple forward passes for stability\n",
    "                n_forwards = 3\n",
    "                batch_preds = []\n",
    "                \n",
    "                for _ in range(n_forwards):\n",
    "                    logits, recon = model(inputs)\n",
    "                    batch_preds.append(F.softmax(logits, dim=1))\n",
    "                \n",
    "                # Average predictions\n",
    "                avg_preds = torch.stack(batch_preds).mean(0)\n",
    "                loss = criterion(avg_preds.log(), targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = avg_preds.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Smooth validation accuracy\n",
    "        validation_window.append(val_acc)\n",
    "        if len(validation_window) > window_size:\n",
    "            validation_window.pop(0)\n",
    "        smoothed_val_acc = sum(validation_window) / len(validation_window)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(smoothed_val_acc)  # Store smoothed validation accuracy\n",
    "        \n",
    "        # Save best model based on smoothed validation accuracy\n",
    "        if smoothed_val_acc > best_val_acc:\n",
    "            best_val_acc = smoothed_val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {smoothed_val_acc:.2f}%')\n",
    "    \n",
    "    # Plot smooth training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    def smooth_curve(points, factor=0.9):  # Increased smoothing factor\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                previous = smoothed_points[-1]\n",
    "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(smooth_curve(train_losses), label='Training Loss')\n",
    "    plt.plot(smooth_curve(val_losses), label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(smooth_curve(train_accs), label='Training Accuracy')\n",
    "    plt.plot(smooth_curve(val_accs), label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/180 [00:00<?, ?it/s]C:\\Users\\eshwa\\AppData\\Local\\Temp\\ipykernel_24264\\1028220634.py:37: UserWarning: Using a target size (torch.Size([16, 4, 1, 1])) that is different to the input size (torch.Size([16, 4, 1, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  consistency_loss = F.smooth_l1_loss(\n",
      "Epoch 1/50: 100%|██████████| 180/180 [00:43<00:00,  4.10it/s, loss=2.3186, acc=10.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "Train Loss: 2.3142, Train Acc: 10.28%\n",
      "Val Loss: 2.3101, Val Acc: 10.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 180/180 [00:47<00:00,  3.79it/s, loss=2.2987, acc=10.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "Train Loss: 2.3000, Train Acc: 10.66%\n",
      "Val Loss: 2.2813, Val Acc: 10.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 180/180 [00:45<00:00,  3.97it/s, loss=2.2186, acc=16.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "Train Loss: 2.2563, Train Acc: 16.88%\n",
      "Val Loss: 2.2321, Val Acc: 14.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 180/180 [00:41<00:00,  4.34it/s, loss=2.2074, acc=23.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "Train Loss: 2.2023, Train Acc: 23.06%\n",
      "Val Loss: 2.1688, Val Acc: 19.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 180/180 [00:28<00:00,  6.38it/s, loss=2.1148, acc=28.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "Train Loss: 2.1295, Train Acc: 28.02%\n",
      "Val Loss: 2.0947, Val Acc: 25.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 180/180 [00:28<00:00,  6.23it/s, loss=2.0523, acc=29.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "Train Loss: 2.0621, Train Acc: 29.24%\n",
      "Val Loss: 2.0311, Val Acc: 28.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 180/180 [00:32<00:00,  5.59it/s, loss=1.8396, acc=33.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "Train Loss: 1.9941, Train Acc: 33.47%\n",
      "Val Loss: 1.9630, Val Acc: 30.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 180/180 [00:32<00:00,  5.61it/s, loss=2.0499, acc=35.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "Train Loss: 1.9464, Train Acc: 35.24%\n",
      "Val Loss: 1.9093, Val Acc: 33.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 180/180 [00:33<00:00,  5.38it/s, loss=1.7561, acc=36.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "Train Loss: 1.9018, Train Acc: 36.70%\n",
      "Val Loss: 1.8752, Val Acc: 35.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 180/180 [00:34<00:00,  5.20it/s, loss=1.7558, acc=37.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "Train Loss: 1.8647, Train Acc: 37.15%\n",
      "Val Loss: 1.8487, Val Acc: 36.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 180/180 [00:37<00:00,  4.77it/s, loss=1.7935, acc=39.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "Train Loss: 1.8343, Train Acc: 39.83%\n",
      "Val Loss: 1.8136, Val Acc: 38.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 180/180 [00:36<00:00,  4.95it/s, loss=1.7040, acc=40.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "Train Loss: 1.8067, Train Acc: 40.38%\n",
      "Val Loss: 1.7863, Val Acc: 40.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 180/180 [00:36<00:00,  4.91it/s, loss=1.7784, acc=42.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "Train Loss: 1.7829, Train Acc: 42.57%\n",
      "Val Loss: 1.7601, Val Acc: 42.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 180/180 [00:37<00:00,  4.80it/s, loss=1.6540, acc=43.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "Train Loss: 1.7657, Train Acc: 43.47%\n",
      "Val Loss: 1.7351, Val Acc: 44.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 180/180 [00:38<00:00,  4.71it/s, loss=1.6707, acc=45.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "Train Loss: 1.7364, Train Acc: 45.24%\n",
      "Val Loss: 1.7128, Val Acc: 46.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 180/180 [00:39<00:00,  4.58it/s, loss=1.6786, acc=45.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "Train Loss: 1.7256, Train Acc: 45.62%\n",
      "Val Loss: 1.7022, Val Acc: 47.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 180/180 [00:40<00:00,  4.46it/s, loss=1.6145, acc=47.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "Train Loss: 1.7204, Train Acc: 47.15%\n",
      "Val Loss: 1.7028, Val Acc: 48.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 180/180 [00:41<00:00,  4.35it/s, loss=1.6307, acc=48.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "Train Loss: 1.7188, Train Acc: 48.58%\n",
      "Val Loss: 1.7096, Val Acc: 49.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 180/180 [00:44<00:00,  4.02it/s, loss=1.7693, acc=48.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "Train Loss: 1.7184, Train Acc: 48.06%\n",
      "Val Loss: 1.6988, Val Acc: 50.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 180/180 [00:44<00:00,  4.08it/s, loss=1.7463, acc=49.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "Train Loss: 1.7147, Train Acc: 49.58%\n",
      "Val Loss: 1.6876, Val Acc: 50.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 180/180 [00:44<00:00,  4.05it/s, loss=1.7004, acc=50.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "Train Loss: 1.7126, Train Acc: 50.59%\n",
      "Val Loss: 1.6851, Val Acc: 51.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 180/180 [00:45<00:00,  3.94it/s, loss=1.7288, acc=49.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "Train Loss: 1.7089, Train Acc: 49.51%\n",
      "Val Loss: 1.6753, Val Acc: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 180/180 [00:46<00:00,  3.89it/s, loss=1.6574, acc=49.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "Train Loss: 1.7044, Train Acc: 49.97%\n",
      "Val Loss: 1.6865, Val Acc: 53.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 180/180 [00:47<00:00,  3.82it/s, loss=1.6972, acc=49.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "Train Loss: 1.7045, Train Acc: 49.90%\n",
      "Val Loss: 1.6615, Val Acc: 54.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 180/180 [00:48<00:00,  3.74it/s, loss=1.6020, acc=50.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "Train Loss: 1.6928, Train Acc: 50.94%\n",
      "Val Loss: 1.6645, Val Acc: 54.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 180/180 [00:52<00:00,  3.44it/s, loss=1.6674, acc=50.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50:\n",
      "Train Loss: 1.7002, Train Acc: 50.52%\n",
      "Val Loss: 1.6602, Val Acc: 53.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 180/180 [00:51<00:00,  3.51it/s, loss=1.7864, acc=51.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50:\n",
      "Train Loss: 1.7036, Train Acc: 51.81%\n",
      "Val Loss: 1.6631, Val Acc: 52.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 180/180 [00:53<00:00,  3.38it/s, loss=1.7407, acc=50.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50:\n",
      "Train Loss: 1.7056, Train Acc: 50.83%\n",
      "Val Loss: 1.6626, Val Acc: 52.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 180/180 [00:54<00:00,  3.32it/s, loss=1.5787, acc=50.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50:\n",
      "Train Loss: 1.7072, Train Acc: 50.59%\n",
      "Val Loss: 1.6707, Val Acc: 53.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 180/180 [00:54<00:00,  3.30it/s, loss=1.7024, acc=51.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50:\n",
      "Train Loss: 1.7150, Train Acc: 51.35%\n",
      "Val Loss: 1.6820, Val Acc: 54.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 180/180 [00:53<00:00,  3.35it/s, loss=1.6801, acc=51.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50:\n",
      "Train Loss: 1.7209, Train Acc: 51.63%\n",
      "Val Loss: 1.6746, Val Acc: 56.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 180/180 [00:59<00:00,  3.05it/s, loss=1.9172, acc=50.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50:\n",
      "Train Loss: 1.7269, Train Acc: 50.94%\n",
      "Val Loss: 1.6855, Val Acc: 56.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 180/180 [00:57<00:00,  3.12it/s, loss=1.8862, acc=50.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50:\n",
      "Train Loss: 1.7318, Train Acc: 50.49%\n",
      "Val Loss: 1.6738, Val Acc: 56.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 180/180 [00:57<00:00,  3.12it/s, loss=1.9257, acc=50.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50:\n",
      "Train Loss: 1.7349, Train Acc: 50.73%\n",
      "Val Loss: 1.6681, Val Acc: 56.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 180/180 [00:57<00:00,  3.13it/s, loss=1.8514, acc=50.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50:\n",
      "Train Loss: 1.7379, Train Acc: 50.94%\n",
      "Val Loss: 1.6543, Val Acc: 55.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 180/180 [01:01<00:00,  2.93it/s, loss=1.8284, acc=51.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50:\n",
      "Train Loss: 1.7424, Train Acc: 51.15%\n",
      "Val Loss: 1.6846, Val Acc: 55.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 180/180 [01:00<00:00,  2.97it/s, loss=1.6815, acc=50.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50:\n",
      "Train Loss: 1.7513, Train Acc: 50.24%\n",
      "Val Loss: 1.7143, Val Acc: 55.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 180/180 [01:00<00:00,  2.98it/s, loss=1.8307, acc=50.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50:\n",
      "Train Loss: 1.7580, Train Acc: 50.38%\n",
      "Val Loss: 1.7083, Val Acc: 55.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 180/180 [01:01<00:00,  2.91it/s, loss=1.9040, acc=49.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50:\n",
      "Train Loss: 1.7724, Train Acc: 49.55%\n",
      "Val Loss: 1.7247, Val Acc: 56.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 180/180 [01:02<00:00,  2.89it/s, loss=1.7042, acc=47.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50:\n",
      "Train Loss: 1.7758, Train Acc: 47.78%\n",
      "Val Loss: 1.7351, Val Acc: 55.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 180/180 [01:03<00:00,  2.85it/s, loss=1.8246, acc=48.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50:\n",
      "Train Loss: 1.7840, Train Acc: 48.82%\n",
      "Val Loss: 1.7321, Val Acc: 55.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 180/180 [01:04<00:00,  2.80it/s, loss=1.7877, acc=47.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50:\n",
      "Train Loss: 1.7911, Train Acc: 47.95%\n",
      "Val Loss: 1.7446, Val Acc: 54.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 180/180 [01:05<00:00,  2.76it/s, loss=1.7617, acc=47.71%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50:\n",
      "Train Loss: 1.7988, Train Acc: 47.71%\n",
      "Val Loss: 1.7212, Val Acc: 53.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 180/180 [01:06<00:00,  2.70it/s, loss=1.6444, acc=47.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50:\n",
      "Train Loss: 1.8062, Train Acc: 47.47%\n",
      "Val Loss: 1.7592, Val Acc: 52.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 180/180 [01:59<00:00,  1.51it/s, loss=1.7651, acc=48.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50:\n",
      "Train Loss: 1.8048, Train Acc: 48.61%\n",
      "Val Loss: 1.7579, Val Acc: 52.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 180/180 [01:08<00:00,  2.64it/s, loss=1.7490, acc=46.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50:\n",
      "Train Loss: 1.8188, Train Acc: 46.46%\n",
      "Val Loss: 1.7615, Val Acc: 52.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 180/180 [01:09<00:00,  2.58it/s, loss=1.7499, acc=47.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50:\n",
      "Train Loss: 1.8199, Train Acc: 47.78%\n",
      "Val Loss: 1.7808, Val Acc: 52.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 180/180 [01:11<00:00,  2.52it/s, loss=2.1318, acc=45.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50:\n",
      "Train Loss: 1.8355, Train Acc: 45.90%\n",
      "Val Loss: 1.7752, Val Acc: 52.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 180/180 [01:16<00:00,  2.36it/s, loss=1.7949, acc=45.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50:\n",
      "Train Loss: 1.8445, Train Acc: 45.00%\n",
      "Val Loss: 1.7862, Val Acc: 52.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 180/180 [01:19<00:00,  2.26it/s, loss=1.9107, acc=45.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50:\n",
      "Train Loss: 1.8461, Train Acc: 45.28%\n",
      "Val Loss: 1.8023, Val Acc: 52.08%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = EEGClassifier(\n",
    "    input_channels=1,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# Train with smaller batch size for stability\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,  # Reduced batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,  # Reduced batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trained_model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
